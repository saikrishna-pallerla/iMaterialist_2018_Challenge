# -*- coding: utf-8 -*-
"""Kaggle_iMat_2018_Fashion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18tDxMVDmsl1uwFMPXqfaO4au9vtMmEDU
"""

''' Kaggle Challenge
https://www.kaggle.com/c/imaterialist-challenge-fashion-2018/data
'''

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import os as os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle/"

##Change directory
# %cd /content/gdrive/My Drive/Kaggle/iMaterialist_Challenge_FGVC5

"""**Step 0:Downloading image files from Kaggle**"""

##Download data
!kaggle competitions download -c imaterialist-challenge-fashion-2018

!ls

#unzipping the zip files and deleting the zip files
!unzip \*.zip  && rm *.zip

!ls

"""Fast AI - DL_Lesson1

**Step1 : Setting up directories and downloading images**
"""

!curl -s https://course.fast.ai/setup/colab | bash

from google.colab import drive
drive.mount('/content/gdrive')
root_dir = "/content/gdrive/My Drive/"
base_dir = root_dir + 'fastai-v3/'

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

from fastai.vision import *
from fastai.metrics import error_rate
from PIL import Image 
import numpy as np
import pandas as pd
import os
import ast
import time as time
import warnings
import shutil
from sklearn.metrics import fbeta_score
warnings.filterwarnings("ignore")

bs = 64
# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart

kaggle_dir = '/content/gdrive/My Drive/Kaggle'
data_dir = kaggle_dir+'/iMaterialist_Challenge_FGVC5'
curr_dir="/content/gdrive/My Drive/Kaggle/iMaterialist_Challenge_FGVC5/"

# Commented out IPython magic to ensure Python compatibility.
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle/"

##Change directory
# %cd /content/gdrive/My Drive/Kaggle/iMaterialist_Challenge_FGVC5

#Creating directories for train,validation and test
folders=['train_data','validation_data','test_data']
train_dir,valid_dir,test_dir=[curr_dir+"/"+str(i) for i in folders]

def create_dirs(path,dirs):
  [os.mkdir(path+'/'+dir) for dir in dirs if not os.path.exists(path+'/'+dir)]
create_dirs(curr_dir,folders)

##Functions to extract only sample data to work on 
def process_raw_data(path,filename,sample_frac,save=False):
  with open(path+filename+'.json') as f:
    data=json.load(f)
  data_ann=data['annotations']
  data_ann=pd.DataFrame(data_ann)
  data_url=data['images']
  data_url=pd.DataFrame(data_url)

  data_final=pd.merge(data_url,data_ann,how='inner',on='imageId')
  if save==True:
    data_final.to_csv(path+filename+'_labels_all.csv',index=False)
  print('Total records in {} is {}'.format(filename,len(data_final)))

  data_dict=data_final.to_dict('records')
  
  if sample_frac<1:
    frac=int(len(data_ann)*sample_frac)
    print('Sampling {} records in {}'.format(frac,filename))
    random.seed(2)
    sample_ids=random.sample(list(data_ann.index),frac)
    data_ann_f=data_ann.iloc[sample_ids]
    data_final=data_final.loc[data_final['imageId'].isin(data_ann_f['imageId'])]
    if save==True:
      data_final.to_csv(path+filename+'_labels.csv',index=False)
    data_dict=data_final.to_dict('records')
    print('Sampled records in {} is {}'.format(filename,len(data_final)))
  return data_final,data_dict

def process_test_data(path,filename,sample_frac,save=False):
  with open(path+filename+'.json') as f:
    data=json.load(f)
  data_url=data['images']
  data_url=pd.DataFrame(data_url)

  if save==True:
    data_url.to_csv(path+filename+'_labels_all.csv',index=False)
  print('Total records in {} is {}'.format(filename,len(data_url)))

  data_dict=data_url.to_dict('records')
  
  if sample_frac<1:
    frac=int(len(data_url)*sample_frac)
    print('Sampling {} records in {}'.format(frac,filename))
    random.seed(2)
    sample_ids=random.sample(list(data_url.index),frac)
    data_url=data_url.iloc[sample_ids]
    if save==True:
      data_url.to_csv(path+filename+'_labels.csv',index=False)
    data_dict=data_url.to_dict('records')
    print('Sampled records in {} is {}'.format(filename,len(data_final)))
  return data_url,data_dict

## Functions to download train,validation and test images
def download_valid_images(img):
  out_path='/content/gdrive/My Drive/Kaggle/iMaterialist_Challenge_FGVC5/validation_data/'
  imgurl,imgid=img['url'],img['imageId']
  img_data = requests.get(imgurl).content
  with open(out_path+'/'+str(imgid)+".jpg", 'wb') as handler:
    handler.write(img_data)

def download_train_images(img):
  out_path='/content/gdrive/My Drive/Kaggle/iMaterialist_Challenge_FGVC5/train_data_final_all_v4/'
  imgurl,imgid=img['url'],img['imageId']
  img_data = requests.get(imgurl).content
  with open(out_path+str(imgid)+".jpg", 'wb') as handler:
    handler.write(img_data)

def download_test_images(img):
  out_path='/content/gdrive/My Drive/Kaggle/iMaterialist_Challenge_FGVC5/test_data/'
  imgurl,imgid=img['url'],img['imageId']
  img_data = requests.get(imgurl).content
  with open(out_path+'/'+str(imgid)+".jpg", 'wb') as handler:
    handler.write(img_data)

def show_image(path,filename): 
  # Read image 
  img = Image.open(path+filename)     
  # Output Images 
  return img

validation_labels,validation_dict=process_raw_data(curr_dir,'validation',1)
train_labels,train_dict=process_raw_data(curr_dir,'train',0.2)

test_labels,test_dict=process_test_data(curr_dir,'test',sample_frac=1)

#Run multiprocessing to download "validation" images
import multiprocessing
from multiprocessing import Pool
from tqdm import tqdm
pool = Pool(processes=12)
print('Length of validation images is {}'.format(len(validation_labels)))
with tqdm(total=len(validation_labels)) as progress_bar:
  for _ in pool.imap_unordered(download_valid_images,validation_dict):
    progress_bar.update(1)

#Run multiprocessing to download "train" images
import multiprocessing
from multiprocessing import Pool
from tqdm import tqdm
pool = Pool(processes=12)
print('Length of train images is {}'.format(len(train_labels)))
with tqdm(total=len(train_labels)) as progress_bar:
  for _ in pool.imap_unordered(download_train_images,train_dict):
    progress_bar.update(1)

#Run multiprocessing to download "test" images
import multiprocessing
from multiprocessing import Pool
from tqdm import tqdm
pool = Pool(processes=12)
print('Length of test images is {}'.format(len(test_labels)))
with tqdm(total=len(test_labels)) as progress_bar:
  for _ in pool.imap_unordered(download_test_images,test_dict):
    progress_bar.update(1)

##Add 'valid_' as prefix to each image file in validation folder
source=curr_dir+'validation_data/'
for filename in os.listdir(source):
  src=source+filename
  dest=source+'valid_'+filename
  os.rename(src,dest)
  
##Add 'test_' as prefix to each image file in test folder
source=curr_dir+'test_data/'
for filename in os.listdir(source):
  src=source+filename
  dest=source+'test_'+filename
  os.rename(src,dest)

"""**Step 2: Image Data Processing**"""

train_labels=pd.read_csv(curr_dir+'train_labels.csv')
validation_labels=pd.read_csv(curr_dir+'validation_labels_all.csv')
test_labels=pd.read_csv(curr_dir+'test_labels_all.csv')

validation_labels['imageId1']='valid_'+validation_labels['imageId'].astype(str)
validation_labels.drop(columns='imageId',inplace=True)
validation_labels.rename(columns={'imageId1':'imageId'},inplace=True)

train_dict=train_labels.to_dict('records')
validation_dict=validation_labels.to_dict('records')

train_labels['is_valid']=0
validation_labels['is_valid']=1
all_labels=pd.concat([train_labels,validation_labels],ignore_index=True)

all_labels['filename']='train_data_final_all/'+all_labels['imageId'].astype(str)+'.jpg'
all_labels.loc[all_labels['is_valid']==1,'filename']='validation_data/'+all_labels['imageId'].astype(str)+'.jpg'
all_labels['labelId']=all_labels['labelId'].str.replace(']',"")
all_labels['labelId']=all_labels['labelId'].str.replace('[',"")
all_labels['labelId']=all_labels['labelId'].str.replace(" ","")
all_labels['labelId']=all_labels['labelId'].apply(lambda x:x.replace("'",""))
all_labels_final=all_labels[['filename','labelId','is_valid']]

##Working on a sample data for train and validation
sample_train_labels2=pd.DataFrame()
sample_train_labels3=pd.DataFrame()
sample_train_labels4=pd.DataFrame()
sample_train_labels1=train_labels[0:7000].reset_index(drop=True)
sample_train_labels2=train_labels[35000:38000].reset_index(drop=True)
sample_train_labels3=train_labels[52000:54000].reset_index(drop=True)
sample_train_labels4=train_labels[69000:70000].reset_index(drop=True)
sample_train_labels=pd.concat([sample_train_labels1,sample_train_labels2,sample_train_labels3,sample_train_labels4],ignore_index=True)
sample_valid_labels=validation_labels[0:1500].reset_index(drop=True)

sample_train_dict=sample_train_labels.to_dict('records')
sample_validation_dict=sample_valid_labels.to_dict('records')

sample_labels=pd.concat([sample_train_labels,sample_valid_labels],ignore_index=True)

sample_labels['filename']='train_data_final_all/'+sample_labels['imageId'].astype(str)+'.jpg'
sample_labels.loc[sample_labels['is_valid']==1,'filename']='validation_data/'+sample_labels['imageId'].astype(str)+'.jpg'
sample_labels['labelId']=sample_labels['labelId'].str.replace(']',"")
sample_labels['labelId']=sample_labels['labelId'].str.replace('[',"")
sample_labels['labelId']=sample_labels['labelId'].str.replace(" ","")
sample_labels['labelId']=sample_labels['labelId'].apply(lambda x:x.replace("'",""))
sample_labels_final=sample_labels[['filename','labelId','is_valid']]

a=[]
for j in sample_labels_final.loc[sample_labels_final['is_valid']==0,'labelId']:
  for k in j.split(','):
    a.append(k)
len(set(a))

#Run multiprocessing to download "train" images
# os.mkdir(curr_dir+'train_data_final_all/')

import multiprocessing
from multiprocessing import Pool
from tqdm import tqdm
pool = Pool(processes=12)
print('Length of train images is {}'.format(len(sample_train_labels)))
with tqdm(total=len(sample_train_labels)) as progress_bar:
  for _ in pool.imap_unordered(download_train_images,sample_train_dict):
    progress_bar.update(1)

"""**Step 3: Using Resnet50 architecture with pretrained weights on 128*128 size images**"""

src=(ImageList.from_df(sample_labels_final,path=curr_dir,cols='filename').split_from_df('is_valid').label_from_df(cols='labelId',label_delim=','))
tfms=get_transforms(do_flip=True)

data=(src.transform(tfms,size=128).databunch(bs=256).normalize(imagenet_stats))

len(data.train_ds.items),len(data.valid_ds.items),len(set(data.classes))

data.show_batch(rows=3)

data.show_batch(DatasetType='valid_ds',rows=3)

show_image(curr_dir,filename='train_data_final_all/12.jpg')

acc_02 = partial(accuracy_thresh, thresh=0.2)
f_score = partial(fbeta, thresh=0.2,beta=1)
learn = cnn_learner(data, models.resnet50, metrics=[acc_02, f_score])

learn.lr_find()

learn.recorder.plot()

lr = 0.06
learn.fit_one_cycle(5, slice(lr))

learn.save('phase1-128-s1-rn50')

"""**Step 4: Fine tune the model weights for all layer groups on 128*128 size images**"""

##Fine tune the model
learn.unfreeze()
learn.lr_find()

learn.recorder.plot()

lr=0.06
learn.fit_one_cycle(5, slice(1e-4, lr/5))

learn.save('phase1-128-s2-rn50')

learn.export('export_phase1_128_s2.pkl')

"""**Step 5: Update the model weights on 256*256 size images**"""

learn=load_learner(curr_dir,'export_phase1_128_s2.pkl')

data=(src.transform(tfms,size=256).databunch(bs=64).normalize(imagenet_stats))
learn.data=data

learn.lr_find()

learn.recorder.plot()

learn.fit_one_cycle(5, slice(1e-3))

learn.save('phase1-256-rn50')

learn.export('export_phase1_256.pkl')

"""**Step 6: Use pretrained weights from Phase1 results and train the model on 128*128 size images**"""

##Working on a sample data for train and validation
sample_train_labels2=pd.DataFrame()
sample_train_labels3=pd.DataFrame()
sample_train_labels4=pd.DataFrame()
sample_train_labels1=train_labels[20000:55000].reset_index(drop=True)
sample_train_labels4=train_labels[69000:70000].reset_index(drop=True)
sample_train_labels=pd.concat([sample_train_labels1,sample_train_labels2,sample_train_labels3,sample_train_labels4],ignore_index=True)
sample_valid_labels=validation_labels[1500:5000].reset_index(drop=True)

sample_train_dict=sample_train_labels.to_dict('records')
sample_validation_dict=sample_valid_labels.to_dict('records')

sample_labels=pd.concat([sample_train_labels,sample_valid_labels],ignore_index=True)

sample_labels['filename']='train_data_final_all_v3/'+sample_labels['imageId'].astype(str)+'.jpg'
sample_labels.loc[sample_labels['is_valid']==1,'filename']='validation_data/'+sample_labels['imageId'].astype(str)+'.jpg'
sample_labels['labelId']=sample_labels['labelId'].str.replace(']',"")
sample_labels['labelId']=sample_labels['labelId'].str.replace('[',"")
sample_labels['labelId']=sample_labels['labelId'].str.replace(" ","")
sample_labels['labelId']=sample_labels['labelId'].apply(lambda x:x.replace("'",""))
sample_labels_final=sample_labels[['filename','labelId','is_valid']]

a=[]
for j in sample_labels_final.loc[sample_labels_final['is_valid']==0,'labelId']:
  for k in j.split(','):
    a.append(k)
len(set(a))

#Run multiprocessing to download "train" images
os.mkdir(curr_dir+'train_data_final_all_v3/')

import multiprocessing
from multiprocessing import Pool
from tqdm import tqdm
pool = Pool(processes=12)
print('Length of train images is {}'.format(len(sample_train_labels)))
with tqdm(total=len(sample_train_labels)) as progress_bar:
  for _ in pool.imap_unordered(download_train_images,sample_train_dict):
    progress_bar.update(1)

src=(ImageList.from_df(sample_labels_final,path=curr_dir,cols='filename').split_from_df('is_valid').label_from_df(cols='labelId',label_delim=','))
tfms=get_transforms(do_flip=True)

data=(src.transform(tfms,size=128).databunch(bs=256).normalize(imagenet_stats))

acc_02 = partial(accuracy_thresh, thresh=0.2)
f_score = partial(fbeta, thresh=0.2,beta=1)
learn = cnn_learner(data, models.resnet50, metrics=[acc_02, f_score])

learn.load('phase1-256-rn50')

len(data.valid_ds.items),len(data.train_ds.items),learn.data.c,len(learn.data.classes),len(learn.layer_groups)

learn.freeze()
learn.lr_find()

learn.recorder.plot()

lr=3e-3
learn.fit_one_cycle(4, slice(lr))

learn.save('phase2-128-s1-rn50')

"""**Step 7: Fine tune the model weights for all layer groups on 128*128 size images**"""

##Fine tune the model
learn.unfreeze()
learn.lr_find()

learn.recorder.plot()

lr=3e-3
learn.fit_one_cycle(4, slice(1e-5,lr/5))

learn.save('phase2-128-s2-rn50')

learn.export('export_phase2_128.pkl')

"""**Step 8: Freeze all layer groups except the last one and update the model weights on 256*256 size images**"""

data=(src.transform(tfms,size=256).databunch(bs=64).normalize(imagenet_stats))
learn.data=data

learn.freeze()
learn.lr_find()

learn.recorder.plot()

lr=7e-6
learn.fit_one_cycle(4,slice(lr))

learn.save('phase2-256-s1-rn50')

"""**Step 9: Fine tune the model weights for all layer groups on 256*256 size images**"""

##Fine tune the model
learn.unfreeze()
learn.lr_find()

learn.recorder.plot()

learn.fit_one_cycle(4,slice(5e-5,5e-4))

learn.save('phase2-256-s2-rn50')

learn.export('export_phase2_256.pkl')

"""**Step 10: Use pretrained weights from Phase2 results and train the model on 128*128 size images**"""

##Working on a sample data for train and validation
sample_train_labels2=pd.DataFrame()
sample_train_labels3=pd.DataFrame()
sample_train_labels4=pd.DataFrame()
sample_train_labels1=train_labels[100000:130000].reset_index(drop=True)
sample_train_labels4=train_labels[165000:170000].reset_index(drop=True)
sample_train_labels2=train_labels[35000:40000].reset_index(drop=True)
sample_train_labels=pd.concat([sample_train_labels1,sample_train_labels2,sample_train_labels3,sample_train_labels4],ignore_index=True)
sample_valid_labels=validation_labels[5000:].reset_index(drop=True)

sample_train_dict=sample_train_labels.to_dict('records')
sample_validation_dict=sample_valid_labels.to_dict('records')

sample_labels=pd.concat([sample_train_labels,sample_valid_labels],ignore_index=True)

sample_labels['filename']='train_data_final_all_v4/'+sample_labels['imageId'].astype(str)+'.jpg'
sample_labels.loc[sample_labels['is_valid']==1,'filename']='validation_data/'+sample_labels['imageId'].astype(str)+'.jpg'
sample_labels['labelId']=sample_labels['labelId'].str.replace(']',"")
sample_labels['labelId']=sample_labels['labelId'].str.replace('[',"")
sample_labels['labelId']=sample_labels['labelId'].str.replace(" ","")
sample_labels['labelId']=sample_labels['labelId'].apply(lambda x:x.replace("'",""))
sample_labels_final=sample_labels[['filename','labelId','is_valid']]

a=[]
for j in sample_labels_final.loc[sample_labels_final['is_valid']==0,'labelId']:
  for k in j.split(','):
    a.append(k)
len(set(a))

#Run multiprocessing to download "train" images
os.mkdir(curr_dir+'train_data_final_all_v4/')

import multiprocessing
from multiprocessing import Pool
from tqdm import tqdm
pool = Pool(processes=12)
print('Length of train images is {}'.format(len(sample_train_labels)))
with tqdm(total=len(sample_train_labels)) as progress_bar:
  for _ in pool.imap_unordered(download_train_images,sample_train_dict):
    progress_bar.update(1)

src=(ImageList.from_df(sample_labels_final,path=curr_dir,cols='filename').split_from_df('is_valid').label_from_df(cols='labelId',label_delim=','))
tfms=get_transforms(do_flip=True)

data=(src.transform(tfms,size=128).databunch(bs=256).normalize(imagenet_stats))

acc_02 = partial(accuracy_thresh, thresh=0.2)
f_score = partial(fbeta, thresh=0.2,beta=1)
learn = cnn_learner(data, models.resnet50, metrics=[acc_02, f_score])

learn.load('phase2-256-s2-rn50')

len(data.valid_ds.items),len(data.train_ds.items),learn.data.c,len(learn.data.classes),len(learn.layer_groups)

learn.freeze()
learn.lr_find()

learn.recorder.plot()

lr=4e-3
learn.fit_one_cycle(4,slice(lr))

learn.save('phase3-128-s1-rn50')

"""**Step 11:Fine tune the model weights for all layer groups on 128*128 size images**"""

##Fine tune the model
learn.unfreeze()
learn.lr_find()

learn.recorder.plot()

lr=4e-3
learn.fit_one_cycle(4, slice(1e-5,lr/5))

learn.recorder.plot_losses()

learn.save('phase3-128-s2-rn50')

learn.export('export_phase3_128.pkl')

"""**Step 12: Freeze all layer groups except the last one and update the model weights on 256*256 size images**"""

data=(src.transform(tfms,size=256).databunch(bs=64).normalize(imagenet_stats))
learn.data=data

learn.freeze()
learn.lr_find()

learn.recorder.plot()

lr=1e-3
learn.fit_one_cycle(5,slice(lr))

learn.save('phase4-256-s1-rn50')

"""**Step 13: Fine tune the model weights for all layer groups on 256*256 size images**"""

learn.unfreeze()
learn.lr_find()

learn.recorder.plot()

learn.fit_one_cycle(5,slice(1e-5,lr/5))

learn.recorder.plot_metrics()

learn.recorder.plot_lr()

learn.recorder.plot_losses()

learn.save('phase4-256-s2-rn50')

learn.export('export_phase3_256.pkl')

"""**Step 14: Inference time predictions on public test data**"""

test_labels=pd.read_csv(curr_dir+'test_labels_all.csv')

test_labels['imageId1']='test_'+test_labels['imageId'].astype(str)
test_labels.drop(columns='imageId',inplace=True)
test_labels.rename(columns={'imageId1':'imageId'},inplace=True)

test_dict=test_labels.to_dict('records')
test_labels['filename']='test_data/'+test_labels['imageId'].astype(str)+'.jpg'
test_labels=test_labels[['filename','url']]

test=ImageList.from_df(test_labels,path=curr_dir,cols='filename')
learn = load_learner(curr_dir, 'export_v4_256_s2.pkl',test=test)
preds, _ = learn.get_preds(ds_type=DatasetType.Test)

thresh=0.18
learn.data.classes_new=[re.sub('\ |\[|\]','',i) for i in learn.data.classes]
labelled_preds = [' '.join([(learn.data.classes_new[i].replace("'",'')) for i,p in enumerate(pred) if p > thresh]) for pred in preds]
labelled_preds1=[' '.join([(k) for k in set(i.split(' '))]) for i in labelled_preds]

f_names=[str(i).rsplit('/')[-1] for i in learn.data.test_ds.items]
fnames=[i.rsplit('_')[1][:-4] for i in f_names]

x1=dict(zip(fnames,labelled_preds1))
df=pd.DataFrame(list(x1.items()),columns=['image_id','label_id'])
df.to_csv(curr_dir+'test_output_final_all_v4_1.csv',index=False)  ##Yieled an private test fbeta score of 0.58014 positioning 38 on the Leaderboard

"""**Step 15: Experiment with different threshold values for differnet classes**"""

validation_labels_final=all_labels_final.loc[all_labels_final['is_valid']==1].reset_index(drop=True)

valid_data=ImageList.from_df(validation_labels_final,path=curr_dir,cols='filename')
learn = load_learner(curr_dir, 'export_phase3_256.pkl',test=valid_data)
preds, _ = learn.get_preds(ds_type=DatasetType.Test)

arr_valid_pred_all=np.array(preds)
arr_valid_y_all=np.zeros((len(validation_labels),228))
for i in range(len(arr_valid_y_all)):
  for j in ast.literal_eval(validation_labels.iloc[i]['labelId']):
    arr_valid_y_all[i][int(j)-1]=1

n_classes=228
def optimise_f2_thresholds(y, p, verbose=True, resolution=100):
  def mf(x):
    p2 = np.zeros_like(p)
    for i in range(n_classes):
      p2[:, i] = (p[:, i] > x[i]).astype(np.int)
    score = fbeta_score(y, p2, beta=1, average='samples')
    return score

  x = [0.2]*n_classes
  for i in range(n_classes):
    best_i2 = 0
    best_score = 0
    for i2 in range(resolution):
      i2 /= resolution
      x[i] = i2
      score = mf(x)
      if score > best_score:
        best_i2 = i2
        best_score = score
    x[i] = best_i2
    if verbose:
      print(i, best_i2, best_score)

  return x

class_best_thresholds=optimise_f2_thresholds(arr_valid_y_all,arr_valid_pred_all)

class_best_thresholds_final=[i if i >0.1 else 0.1 for i in class_best_thresholds]
class_best_thresholds_final1=[i if i <0.2 else 0.2 for i in class_best_thresholds_final]

##Testing Outputs based on new thresholds

learn.data.classes_new=[re.sub('\ |\[|\]','',i) for i in learn.data.classes]
labelled_preds = [' '.join([(learn.data.classes_new[i].replace("'",'')) for i,p in enumerate(pred) if p > class_best_thresholds_final1[i]]) for pred in preds]
labelled_preds1=[' '.join([(k) for k in set(i.split(' '))]) for i in labelled_preds]

f_names=[str(i).rsplit('/')[-1] for i in learn.data.test_ds.items]
fnames=[i.rsplit('_')[1][:-4] for i in f_names]

x1=dict(zip(fnames,labelled_preds1))
df=pd.DataFrame(list(x1.items()),columns=['image_id','label_id'])
df.to_csv(curr_dir+'test_output_final_all_v3_th_adj2.csv',index=False)

###Optimizing for thresholds didn't work and is unaffective